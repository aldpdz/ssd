{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from extra_files import helper as hp\n",
    "from imageio import imwrite, imread\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "data_path = '/home/aldo/Documents/data-cic/'\n",
    "preprocess_path = data_path + 'preprocess_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SSD300 trained with vgg16 backbone trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/aldo/Documents/ssd/data_generator/object_detection_2d_data_generator.py:43: UserWarning: 'BeautifulSoup' module is missing. The XML-parser will be unavailable.\n",
      "  warnings.warn(\"'BeautifulSoup' module is missing. The XML-parser will be unavailable.\")\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters (original SSD300 architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameteres needed for ssd_300() and SSDInputEncoder()\n",
    "\n",
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 1 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales = scales_pascal\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new model with SSD weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Build the Keras model.\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=mean_color,\n",
    "                swap_channels=swap_channels)\n",
    "\n",
    "# 2: Load some weights into the model.\n",
    "\n",
    "# TODO: Set the path to the weights you want to load.\n",
    "# weights_path = '../weights/VGG_ILSVRC_16_layers_fc_reduced.h5'\n",
    "weights_path = '../weights/models/ssd300_adam_vgg16/ssd300_vgg_pascal_epoch-202_loss-3.2085_val_loss-3.2531.h5'\n",
    "\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "# 3: Instantiate an optimizer and the SSD loss function and compile the model.\n",
    "#    If you want to follow the original Caffe implementation, use the preset SGD\n",
    "#    optimizer, otherwise I'd recommend the commented-out Adam optimizer.\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 300, 300, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "identity_layer (Lambda)         (None, 300, 300, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_mean_normalization (Lambd (None, 300, 300, 3)  0           identity_layer[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_channel_swap (Lambda)     (None, 300, 300, 3)  0           input_mean_normalization[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1 (Conv2D)                (None, 300, 300, 64) 1792        input_channel_swap[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1_2 (Conv2D)                (None, 300, 300, 64) 36928       conv1_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 150, 150, 64) 0           conv1_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1 (Conv2D)                (None, 150, 150, 128 73856       pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2 (Conv2D)                (None, 150, 150, 128 147584      conv2_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)            (None, 75, 75, 128)  0           conv2_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1 (Conv2D)                (None, 75, 75, 256)  295168      pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2 (Conv2D)                (None, 75, 75, 256)  590080      conv3_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3 (Conv2D)                (None, 75, 75, 256)  590080      conv3_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 38, 38, 256)  0           conv3_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1 (Conv2D)                (None, 38, 38, 512)  1180160     pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2 (Conv2D)                (None, 38, 38, 512)  2359808     conv4_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3 (Conv2D)                (None, 38, 38, 512)  2359808     conv4_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool4 (MaxPooling2D)            (None, 19, 19, 512)  0           conv4_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1 (Conv2D)                (None, 19, 19, 512)  2359808     pool4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2 (Conv2D)                (None, 19, 19, 512)  2359808     conv5_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3 (Conv2D)                (None, 19, 19, 512)  2359808     conv5_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 19, 19, 512)  0           conv5_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "fc6 (Conv2D)                    (None, 19, 19, 1024) 4719616     pool5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fc7 (Conv2D)                    (None, 19, 19, 1024) 1049600     fc6[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv6_1 (Conv2D)                (None, 19, 19, 256)  262400      fc7[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv6_padding (ZeroPadding2D)   (None, 21, 21, 256)  0           conv6_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2 (Conv2D)                (None, 10, 10, 512)  1180160     conv6_padding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv7_1 (Conv2D)                (None, 10, 10, 128)  65664       conv6_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv7_padding (ZeroPadding2D)   (None, 12, 12, 128)  0           conv7_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2 (Conv2D)                (None, 5, 5, 256)    295168      conv7_padding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv8_1 (Conv2D)                (None, 5, 5, 128)    32896       conv7_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2 (Conv2D)                (None, 3, 3, 256)    295168      conv8_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv9_1 (Conv2D)                (None, 3, 3, 128)    32896       conv8_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm (L2Normalization)  (None, 38, 38, 512)  512         conv4_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2 (Conv2D)                (None, 1, 1, 256)    295168      conv9_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_conf (Conv2D) (None, 38, 38, 8)    36872       conv4_3_norm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_conf (Conv2D)          (None, 19, 19, 12)   110604      fc7[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_conf (Conv2D)      (None, 10, 10, 12)   55308       conv6_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_conf (Conv2D)      (None, 5, 5, 12)     27660       conv7_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_conf (Conv2D)      (None, 3, 3, 8)      18440       conv8_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_conf (Conv2D)      (None, 1, 1, 8)      18440       conv9_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_loc (Conv2D)  (None, 38, 38, 16)   73744       conv4_3_norm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_loc (Conv2D)           (None, 19, 19, 24)   221208      fc7[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_loc (Conv2D)       (None, 10, 10, 24)   110616      conv6_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_loc (Conv2D)       (None, 5, 5, 24)     55320       conv7_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_loc (Conv2D)       (None, 3, 3, 16)     36880       conv8_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_loc (Conv2D)       (None, 1, 1, 16)     36880       conv9_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_conf_reshape  (None, 5776, 2)      0           conv4_3_norm_mbox_conf[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_conf_reshape (Reshape) (None, 2166, 2)      0           fc7_mbox_conf[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_conf_reshape (Resh (None, 600, 2)       0           conv6_2_mbox_conf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_conf_reshape (Resh (None, 150, 2)       0           conv7_2_mbox_conf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_conf_reshape (Resh (None, 36, 2)        0           conv8_2_mbox_conf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_conf_reshape (Resh (None, 4, 2)         0           conv9_2_mbox_conf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_priorbox (Anc (None, 38, 38, 4, 8) 0           conv4_3_norm_mbox_loc[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_priorbox (AnchorBoxes) (None, 19, 19, 6, 8) 0           fc7_mbox_loc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_priorbox (AnchorBo (None, 10, 10, 6, 8) 0           conv6_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_priorbox (AnchorBo (None, 5, 5, 6, 8)   0           conv7_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_priorbox (AnchorBo (None, 3, 3, 4, 8)   0           conv8_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_priorbox (AnchorBo (None, 1, 1, 4, 8)   0           conv9_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mbox_conf (Concatenate)         (None, 8732, 2)      0           conv4_3_norm_mbox_conf_reshape[0]\n",
      "                                                                 fc7_mbox_conf_reshape[0][0]      \n",
      "                                                                 conv6_2_mbox_conf_reshape[0][0]  \n",
      "                                                                 conv7_2_mbox_conf_reshape[0][0]  \n",
      "                                                                 conv8_2_mbox_conf_reshape[0][0]  \n",
      "                                                                 conv9_2_mbox_conf_reshape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_loc_reshape ( (None, 5776, 4)      0           conv4_3_norm_mbox_loc[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_loc_reshape (Reshape)  (None, 2166, 4)      0           fc7_mbox_loc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_loc_reshape (Resha (None, 600, 4)       0           conv6_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_loc_reshape (Resha (None, 150, 4)       0           conv7_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_loc_reshape (Resha (None, 36, 4)        0           conv8_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_loc_reshape (Resha (None, 4, 4)         0           conv9_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_priorbox_resh (None, 5776, 8)      0           conv4_3_norm_mbox_priorbox[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_priorbox_reshape (Resh (None, 2166, 8)      0           fc7_mbox_priorbox[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_priorbox_reshape ( (None, 600, 8)       0           conv6_2_mbox_priorbox[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_priorbox_reshape ( (None, 150, 8)       0           conv7_2_mbox_priorbox[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_priorbox_reshape ( (None, 36, 8)        0           conv8_2_mbox_priorbox[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_priorbox_reshape ( (None, 4, 8)         0           conv9_2_mbox_priorbox[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mbox_conf_softmax (Activation)  (None, 8732, 2)      0           mbox_conf[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mbox_loc (Concatenate)          (None, 8732, 4)      0           conv4_3_norm_mbox_loc_reshape[0][\n",
      "                                                                 fc7_mbox_loc_reshape[0][0]       \n",
      "                                                                 conv6_2_mbox_loc_reshape[0][0]   \n",
      "                                                                 conv7_2_mbox_loc_reshape[0][0]   \n",
      "                                                                 conv8_2_mbox_loc_reshape[0][0]   \n",
      "                                                                 conv9_2_mbox_loc_reshape[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "mbox_priorbox (Concatenate)     (None, 8732, 8)      0           conv4_3_norm_mbox_priorbox_reshap\n",
      "                                                                 fc7_mbox_priorbox_reshape[0][0]  \n",
      "                                                                 conv6_2_mbox_priorbox_reshape[0][\n",
      "                                                                 conv7_2_mbox_priorbox_reshape[0][\n",
      "                                                                 conv8_2_mbox_priorbox_reshape[0][\n",
      "                                                                 conv9_2_mbox_priorbox_reshape[0][\n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, 8732, 14)     0           mbox_conf_softmax[0][0]          \n",
      "                                                                 mbox_loc[0][0]                   \n",
      "                                                                 mbox_priorbox[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 23,745,908\n",
      "Trainable params: 23,745,908\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images into memory: 100%|██████████| 210/210 [00:08<00:00, 24.26it/s]\n",
      "Loading images into memory: 100%|██████████| 45/45 [00:01<00:00, 27.30it/s]\n",
      "Number of images in the training dataset:\t   210\n",
      "Number of images in the validation dataset:\t    45\n"
     ]
    }
   ],
   "source": [
    "# 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
    "\n",
    "# Optional: If you have enough memory, consider loading the images into memory for the reasons explained above.\n",
    "\n",
    "train_dataset = DataGenerator(load_images_into_memory=True, hdf5_dataset_path=None)\n",
    "val_dataset = DataGenerator(load_images_into_memory=True, hdf5_dataset_path=None)\n",
    "\n",
    "# 2: Parse the image and label lists for the training and validation datasets.\n",
    "\n",
    "# TODO: Set the paths to your dataset here.\n",
    "\n",
    "# Images\n",
    "# images_dir = data_path + 'udacity_driving_datasets'\n",
    "# images_dir = data_path + 'pascal_dataset'\n",
    "images_dir = data_path + 'images'\n",
    "\n",
    "# Ground truth\n",
    "# train_labels_filename = preprocess_path + '/udc.csv'\n",
    "# train_labels_filename = preprocess_path + '/cic_pascal_train.csv'\n",
    "# val_labels_filename   = preprocess_path + '/cic_pascal_val.csv'\n",
    "# train_labels_filename = preprocess_path + '/pascal_train.csv'\n",
    "# val_labels_filename   = preprocess_path + '/pascal_val.csv'\n",
    "train_labels_filename = preprocess_path + '/cic_train.csv'\n",
    "val_labels_filename   = preprocess_path + '/cic_val.csv'\n",
    "\n",
    "train_dataset.parse_csv(images_dir=images_dir,\n",
    "                        labels_filename=train_labels_filename,\n",
    "                        input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'], # This is the order of the first six columns in the CSV file that contains the labels for your dataset. If your labels are in XML format, maybe the XML parser will be helpful, check the documentation.\n",
    "                        include_classes='all')\n",
    "\n",
    "val_dataset.parse_csv(images_dir=images_dir,\n",
    "                      labels_filename=val_labels_filename,\n",
    "                      input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'],\n",
    "                      include_classes='all')\n",
    "\n",
    "# Optional: Convert the dataset into an HDF5 dataset. This will require more disk space, but will\n",
    "# speed up the training. Doing this is not relevant in case you activated the `load_images_into_memory`\n",
    "# option in the constructor, because in that cas the images are in memory already anyway. If you don't\n",
    "# want to create HDF5 datasets, comment out the subsequent two function calls.\n",
    "\n",
    "# train_dataset.create_hdf5_dataset(file_path='dataset_udacity_traffic_train.h5',\n",
    "#                                   resize=False,\n",
    "#                                   variable_image_size=True,\n",
    "#                                   verbose=True)\n",
    "\n",
    "# val_dataset.create_hdf5_dataset(file_path='dataset_udacity_traffic_val.h5',\n",
    "#                                 resize=False,\n",
    "#                                 variable_image_size=True,\n",
    "#                                 verbose=True)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training dataset:\t   210\n",
      "Number of images in the validation dataset:\t    45\n"
     ]
    }
   ],
   "source": [
    "# 3: Set the batch size.\n",
    "batch_size = 32 # Change the batch size if you like, or if you run into GPU memory issues.\n",
    "\n",
    "# 4: Set the image transformations for pre-processing and data augmentation options.\n",
    "# For the training generator:\n",
    "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
    "                                            img_width=img_width,\n",
    "                                            background=mean_color)\n",
    "\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [model.get_layer('conv4_3_norm_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('fc7_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv6_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv7_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv8_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv9_2_mbox_conf').output_shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[ssd_data_augmentation],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a learning rate schedule.\n",
    "\n",
    "# def lr_schedule(epoch):\n",
    "#     if epoch < 80:\n",
    "#         return 0.001\n",
    "#     elif epoch < 150:\n",
    "#         return 0.0001\n",
    "#     else:\n",
    "#         return 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model callbacks.\n",
    "\n",
    "# TODO: Set the filepath under which you want to save the model.\n",
    "model_checkpoint = ModelCheckpoint(filepath='/home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "# model_checkpoint.best = 3.1903697716717954\n",
    "\n",
    "csv_logger = CSVLogger(filename=data_path + 'history/ssd300_adam_vgg_pedcic/ssd300_vgg_pascal_other.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0.0,\n",
    "                               patience=10,\n",
    "                               verbose=1)\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                         factor=0.2,\n",
    "                                         patience=8,\n",
    "                                         verbose=1,\n",
    "                                         min_delta=0.001,\n",
    "                                         cooldown=0,\n",
    "                                         min_lr=0.00001)\n",
    "\n",
    "# learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule,\n",
    "#                                                 verbose=1)\n",
    "\n",
    "# terminate_on_nan = TerminateOnNaN()\n",
    "\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             early_stopping,\n",
    "             reduce_learning_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "30/30 [==============================] - 135s 4s/step - loss: 3.9578 - val_loss: 3.4561\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.45613, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-01_loss-3.9687_val_loss-3.4561.h5\n",
      "Epoch 2/300\n",
      "30/30 [==============================] - 130s 4s/step - loss: 3.4259 - val_loss: 3.2059\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.45613 to 3.20593, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-02_loss-3.4219_val_loss-3.2059.h5\n",
      "Epoch 3/300\n",
      "30/30 [==============================] - 127s 4s/step - loss: 3.2547 - val_loss: 3.1133\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.20593 to 3.11325, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-03_loss-3.2552_val_loss-3.1133.h5\n",
      "Epoch 4/300\n",
      "30/30 [==============================] - 126s 4s/step - loss: 3.1755 - val_loss: 3.0977\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.11325 to 3.09773, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-04_loss-3.1835_val_loss-3.0977.h5\n",
      "Epoch 5/300\n",
      "30/30 [==============================] - 129s 4s/step - loss: 3.0911 - val_loss: 3.0264\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.09773 to 3.02635, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-05_loss-3.1098_val_loss-3.0264.h5\n",
      "Epoch 6/300\n",
      "30/30 [==============================] - 132s 4s/step - loss: 3.0675 - val_loss: 3.0503\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3.02635\n",
      "Epoch 7/300\n",
      "30/30 [==============================] - 137s 5s/step - loss: 3.0375 - val_loss: 2.9801\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.02635 to 2.98012, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-07_loss-3.0201_val_loss-2.9801.h5\n",
      "Epoch 8/300\n",
      "30/30 [==============================] - 138s 5s/step - loss: 2.9788 - val_loss: 2.9082\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.98012 to 2.90821, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-08_loss-2.9842_val_loss-2.9082.h5\n",
      "Epoch 9/300\n",
      "30/30 [==============================] - 142s 5s/step - loss: 3.0426 - val_loss: 3.0557\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.90821\n",
      "Epoch 10/300\n",
      "30/30 [==============================] - 134s 4s/step - loss: 2.9933 - val_loss: 3.1503\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.90821\n",
      "Epoch 11/300\n",
      "30/30 [==============================] - 138s 5s/step - loss: 3.0022 - val_loss: 2.8844\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.90821 to 2.88445, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-11_loss-3.0057_val_loss-2.8844.h5\n",
      "Epoch 12/300\n",
      "30/30 [==============================] - 132s 4s/step - loss: 2.9506 - val_loss: 3.0134\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.88445\n",
      "Epoch 13/300\n",
      "30/30 [==============================] - 133s 4s/step - loss: 2.9615 - val_loss: 2.9345\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.88445\n",
      "Epoch 14/300\n",
      "30/30 [==============================] - 134s 4s/step - loss: 2.8801 - val_loss: 2.7462\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.88445 to 2.74615, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-14_loss-2.8647_val_loss-2.7462.h5\n",
      "Epoch 15/300\n",
      "30/30 [==============================] - 132s 4s/step - loss: 2.8569 - val_loss: 2.9253\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.74615\n",
      "Epoch 16/300\n",
      "30/30 [==============================] - 142s 5s/step - loss: 2.9684 - val_loss: 2.9214\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.74615\n",
      "Epoch 17/300\n",
      "30/30 [==============================] - 136s 5s/step - loss: 2.8674 - val_loss: 2.8920\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.74615\n",
      "Epoch 18/300\n",
      "30/30 [==============================] - 127s 4s/step - loss: 2.7110 - val_loss: 2.8057\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.74615\n",
      "Epoch 19/300\n",
      "30/30 [==============================] - 131s 4s/step - loss: 2.8491 - val_loss: 2.8516\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.74615\n",
      "Epoch 20/300\n",
      "30/30 [==============================] - 132s 4s/step - loss: 2.7757 - val_loss: 2.8300\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.74615\n",
      "Epoch 21/300\n",
      "30/30 [==============================] - 126s 4s/step - loss: 2.7802 - val_loss: 2.8513\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.74615\n",
      "Epoch 22/300\n",
      "30/30 [==============================] - 143s 5s/step - loss: 2.7525 - val_loss: 2.7524\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.74615\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 23/300\n",
      "30/30 [==============================] - 137s 5s/step - loss: 2.5946 - val_loss: 2.6724\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.74615 to 2.67236, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-23_loss-2.5863_val_loss-2.6724.h5\n",
      "Epoch 24/300\n",
      "30/30 [==============================] - 139s 5s/step - loss: 2.4417 - val_loss: 2.5976\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.67236 to 2.59760, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-24_loss-2.4451_val_loss-2.5976.h5\n",
      "Epoch 25/300\n",
      "30/30 [==============================] - 134s 4s/step - loss: 2.4588 - val_loss: 2.6199\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.59760\n",
      "Epoch 26/300\n",
      "30/30 [==============================] - 139s 5s/step - loss: 2.4989 - val_loss: 2.6326\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.59760\n",
      "Epoch 27/300\n",
      "30/30 [==============================] - 133s 4s/step - loss: 2.4195 - val_loss: 2.6559\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.59760\n",
      "Epoch 28/300\n",
      "30/30 [==============================] - 130s 4s/step - loss: 2.3640 - val_loss: 2.5517\n",
      "\n",
      "Epoch 00028: val_loss improved from 2.59760 to 2.55170, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-28_loss-2.3611_val_loss-2.5517.h5\n",
      "Epoch 29/300\n",
      "30/30 [==============================] - 135s 4s/step - loss: 2.3044 - val_loss: 2.5512\n",
      "\n",
      "Epoch 00029: val_loss improved from 2.55170 to 2.55119, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-29_loss-2.3105_val_loss-2.5512.h5\n",
      "Epoch 30/300\n",
      "30/30 [==============================] - 131s 4s/step - loss: 2.3991 - val_loss: 2.5176\n",
      "\n",
      "Epoch 00030: val_loss improved from 2.55119 to 2.51758, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-30_loss-2.3949_val_loss-2.5176.h5\n",
      "Epoch 31/300\n",
      "30/30 [==============================] - 136s 5s/step - loss: 2.3177 - val_loss: 2.5419\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.51758\n",
      "Epoch 32/300\n",
      "30/30 [==============================] - 133s 4s/step - loss: 2.3296 - val_loss: 2.5000\n",
      "\n",
      "Epoch 00032: val_loss improved from 2.51758 to 2.50001, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-32_loss-2.3314_val_loss-2.5000.h5\n",
      "Epoch 33/300\n",
      "30/30 [==============================] - 133s 4s/step - loss: 2.3108 - val_loss: 2.5192\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2.50001\n",
      "Epoch 34/300\n",
      "30/30 [==============================] - 133s 4s/step - loss: 2.3458 - val_loss: 2.5819\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2.50001\n",
      "Epoch 35/300\n",
      "30/30 [==============================] - 136s 5s/step - loss: 2.3160 - val_loss: 2.5298\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 2.50001\n",
      "Epoch 36/300\n",
      "30/30 [==============================] - 142s 5s/step - loss: 2.3246 - val_loss: 2.4953\n",
      "\n",
      "Epoch 00036: val_loss improved from 2.50001 to 2.49535, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-36_loss-2.3122_val_loss-2.4953.h5\n",
      "Epoch 37/300\n",
      "30/30 [==============================] - 130s 4s/step - loss: 2.2704 - val_loss: 2.4874\n",
      "\n",
      "Epoch 00037: val_loss improved from 2.49535 to 2.48740, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-37_loss-2.2638_val_loss-2.4874.h5\n",
      "Epoch 38/300\n",
      "30/30 [==============================] - 138s 5s/step - loss: 2.3243 - val_loss: 2.4430\n",
      "\n",
      "Epoch 00038: val_loss improved from 2.48740 to 2.44304, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-38_loss-2.3245_val_loss-2.4430.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/300\n",
      "30/30 [==============================] - 130s 4s/step - loss: 2.2326 - val_loss: 2.4484\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 2.44304\n",
      "Epoch 40/300\n",
      "30/30 [==============================] - 136s 5s/step - loss: 2.2689 - val_loss: 2.4387\n",
      "\n",
      "Epoch 00040: val_loss improved from 2.44304 to 2.43874, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-40_loss-2.2830_val_loss-2.4387.h5\n",
      "Epoch 41/300\n",
      "30/30 [==============================] - 136s 5s/step - loss: 2.2516 - val_loss: 2.4436\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2.43874\n",
      "Epoch 42/300\n",
      "30/30 [==============================] - 131s 4s/step - loss: 2.2526 - val_loss: 2.4691\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 2.43874\n",
      "Epoch 43/300\n",
      "30/30 [==============================] - 132s 4s/step - loss: 2.2812 - val_loss: 2.4354\n",
      "\n",
      "Epoch 00043: val_loss improved from 2.43874 to 2.43536, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-43_loss-2.2703_val_loss-2.4354.h5\n",
      "Epoch 44/300\n",
      "30/30 [==============================] - 133s 4s/step - loss: 2.2939 - val_loss: 2.4461\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.43536\n",
      "Epoch 45/300\n",
      "30/30 [==============================] - 136s 5s/step - loss: 2.2688 - val_loss: 2.4278\n",
      "\n",
      "Epoch 00045: val_loss improved from 2.43536 to 2.42783, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-45_loss-2.2767_val_loss-2.4278.h5\n",
      "Epoch 46/300\n",
      "30/30 [==============================] - 130s 4s/step - loss: 2.1887 - val_loss: 2.5043\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2.42783\n",
      "Epoch 47/300\n",
      "30/30 [==============================] - 127s 4s/step - loss: 2.2066 - val_loss: 2.4086\n",
      "\n",
      "Epoch 00047: val_loss improved from 2.42783 to 2.40859, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-47_loss-2.2159_val_loss-2.4086.h5\n",
      "Epoch 48/300\n",
      "30/30 [==============================] - 133s 4s/step - loss: 2.2646 - val_loss: 2.3561\n",
      "\n",
      "Epoch 00048: val_loss improved from 2.40859 to 2.35615, saving model to /home/aldo/Documents/weights/models/ssd300_adam_vgg16/ssd300_other_pascal_epoch-48_loss-2.2532_val_loss-2.3561.h5\n",
      "Epoch 49/300\n",
      "30/30 [==============================] - 123s 4s/step - loss: 2.2248 - val_loss: 2.5216\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.35615\n",
      "Epoch 50/300\n",
      "30/30 [==============================] - 130s 4s/step - loss: 2.1815 - val_loss: 2.5742\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.35615\n",
      "Epoch 51/300\n",
      "30/30 [==============================] - 128s 4s/step - loss: 2.2154 - val_loss: 2.4577\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.35615\n",
      "Epoch 52/300\n",
      "30/30 [==============================] - 125s 4s/step - loss: 2.1932 - val_loss: 2.4075\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2.35615\n",
      "Epoch 53/300\n",
      "30/30 [==============================] - 130s 4s/step - loss: 2.1888 - val_loss: 2.5062\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.35615\n",
      "Epoch 54/300\n",
      "30/30 [==============================] - 126s 4s/step - loss: 2.0928 - val_loss: 2.4658\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.35615\n",
      "Epoch 55/300\n",
      "30/30 [==============================] - 131s 4s/step - loss: 2.2027 - val_loss: 2.4956\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 2.35615\n",
      "Epoch 56/300\n",
      "30/30 [==============================] - 124s 4s/step - loss: 2.2201 - val_loss: 2.4820\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 2.35615\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 57/300\n",
      "30/30 [==============================] - 133s 4s/step - loss: 2.0899 - val_loss: 2.3678\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 2.35615\n",
      "Epoch 58/300\n",
      "30/30 [==============================] - 126s 4s/step - loss: 2.1144 - val_loss: 2.3772\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 2.35615\n",
      "Epoch 00058: early stopping\n"
     ]
    }
   ],
   "source": [
    "# If you're resuming a previous training, set `initial_epoch` and `final_epoch` accordingly.\n",
    "initial_epoch   = 0\n",
    "final_epoch     = 300\n",
    "steps_per_epoch = 30\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path + 'history/ssd300_adam_vgg_pedcic/ssd300_vgg_pascal_other.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58 entries, 0 to 57\n",
      "Data columns (total 3 columns):\n",
      "epoch       58 non-null int64\n",
      "loss        58 non-null float64\n",
      "val_loss    58 non-null float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 1.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f981798d080>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df['epoch'], df['loss'])\n",
    "plt.plot(df['epoch'], df['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.253070840849215"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint.best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test2]",
   "language": "python",
   "name": "conda-env-test2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
