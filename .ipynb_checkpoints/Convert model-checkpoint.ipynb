{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/aldopedraza/Documentos/ssd/data_generator/object_detection_2d_data_generator.py:43: UserWarning: 'BeautifulSoup' module is missing. The XML-parser will be unavailable.\n",
      "  warnings.warn(\"'BeautifulSoup' module is missing. The XML-parser will be unavailable.\")\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import Adam\n",
    "from imageio import imread\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from models.keras_ssd512 import ssd_512\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = '../models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, height, width):\n",
    "    # 1: Build the Keras model\n",
    "    K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "    if height == 300:\n",
    "        model = ssd_300(image_size=(height, width, 3),\n",
    "                        n_classes=20,\n",
    "                        mode='inference',\n",
    "                        l2_regularization=0.0005,\n",
    "                        scales=[0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05], # The scales for MS COCO are [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05]\n",
    "                        aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
    "                                                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                                 [1.0, 2.0, 0.5],\n",
    "                                                 [1.0, 2.0, 0.5]],\n",
    "                        two_boxes_for_ar1=True,\n",
    "                        steps=[8, 16, 32, 64, 100, 300],\n",
    "                        offsets=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
    "                        clip_boxes=False,\n",
    "                        variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                        normalize_coords=True,\n",
    "                        subtract_mean=[123, 117, 104],\n",
    "                        swap_channels=False,\n",
    "                        confidence_thresh=0.5,\n",
    "                        iou_threshold=0.45,\n",
    "                        top_k=200,\n",
    "                        nms_max_output_size=400)\n",
    "    elif height == 512:\n",
    "        model = ssd_512(image_size=(height, width, 3),\n",
    "                n_classes=20,\n",
    "                mode='inference',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=[0.07, 0.15, 0.3, 0.45, 0.6, 0.75, 0.9, 1.05], # The scales for MS COCO are [0.04, 0.1, 0.26, 0.42, 0.58, 0.74, 0.9, 1.06]\n",
    "                aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5]],\n",
    "               two_boxes_for_ar1=True,\n",
    "               steps=[8, 16, 32, 64, 128, 256, 512],\n",
    "               offsets=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
    "               clip_boxes=False,\n",
    "               variances=[0.1, 0.1, 0.2, 0.2],\n",
    "               normalize_coords=True,\n",
    "               subtract_mean=[123, 117, 104],\n",
    "               swap_channels=False,\n",
    "               confidence_thresh=0.5,\n",
    "               iou_threshold=0.45,\n",
    "               top_k=200,\n",
    "               nms_max_output_size=400)\n",
    "\n",
    "    # 2: Load the trained weights into the model.\n",
    "    # TODO: Set the path of the trained weights.\n",
    "    model.load_weights(model_path, by_name=True)\n",
    "\n",
    "    # 3: Compile the model so that Keras won't complain the next time you load it.\n",
    "\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "    model.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    '''\n",
    "    Load a frozen graph\n",
    "    frozen_grapth_filename: path in the disk\n",
    "    return: tensorflow graph\n",
    "    '''\n",
    "    # We load the protobuf file from the disk and parse it to retrieve the \n",
    "    # unserialized graph_def\n",
    "    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        \n",
    "    # Then, we import the graph_def into a new Graph and returns it \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        # The name var will prefix every op/nodes in your graph\n",
    "        # Since we load everything in a new graph, this is not needed\n",
    "        tf.import_graph_def(graph_def, name=\"prefix\")\n",
    "    return graph\n",
    "\n",
    "def save_graph(name):\n",
    "    '''\n",
    "    Save grapth to load with tensorboard\n",
    "    '''\n",
    "    graph = load_graph(name)\n",
    "\n",
    "    # save data from a grapth to display on tensorboard\n",
    "    # tensorboard --logdir=wholepath\n",
    "    # the path has to be absolute\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        tf.summary.FileWriter('../logs_ssd', sess.graph)\n",
    "\n",
    "def show_operations_graph(graph):\n",
    "    '''\n",
    "    Print operations contained in a graph\n",
    "    '''\n",
    "    for op in graph.get_operations():\n",
    "        print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models and weights\n",
    "model_pascal_voc_07_plus_12_300x300 = '../weights/pascal_voc/pascal_voc_07_plus_12_300x300.h5'\n",
    "model = load_model(model_pascal_voc_07_plus_12_300x300, 300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../models/ssd_model.pb'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save models as .pbtxt or pb\n",
    "tf.train.write_graph(K.get_session().graph, path_models, \"ssd_model.pbtxt\", as_text=True)\n",
    "tf.train.write_graph(K.get_session().graph, path_models, \"ssd_model.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_graph(path_models + '/ssd_model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "sess = keras.backend.get_session()\n",
    "save_path = saver.save(sess, path_models + \"/ssd_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze the Graph\n",
    "In order to convert the graph to tflite we need to freeze it, in this case we'll use the tools provided by tensorflow to do so.\n",
    "\n",
    "We need to specify the graph, the checkpoints, the output name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/ssd_model.ckpt\n",
      "INFO:tensorflow:Froze 71 variables.\n",
      "INFO:tensorflow:Converted 71 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "# Freeze the graph\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "\n",
    "input_graph_path = path_models + '/ssd_model.pb'\n",
    "checkpoint_path = path_models + '/ssd_model.ckpt'\n",
    "input_saver_def_path = \"\"\n",
    "input_binary = True\n",
    "output_node_names = \"decoded_predictions/loop_over_batch/TensorArrayStack/TensorArrayGatherV3\"\n",
    "restore_op_name = \"save/restore_all\"\n",
    "filename_tensor_name = \"save/Const:0\"\n",
    "output_frozen_graph_name = path_models + '/ssd_frozen_model.pb'\n",
    "clear_devices = True\n",
    "\n",
    "\n",
    "freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\n",
    "                          input_binary, checkpoint_path, output_node_names,\n",
    "                          restore_op_name, filename_tensor_name,\n",
    "                          output_frozen_graph_name, clear_devices, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize for inference\n",
    "\n",
    "Optimize for inference will help us to remove those parts of the graph that were used during training.\n",
    "\n",
    "We need the input and output names and the frozen model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# compile graph_transforms once\n",
    "# https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms/\n",
    "bazel build tensorflow/tools/graph_transforms:transform_graph\n",
    "!touch WORKSPACE # bazel needs it to work\n",
    "    \n",
    "bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n",
    "--in_graph=/home/aldopedraza/Documentos/ssd/extra_files/models/ssd_frozen_model.pb \\\n",
    "--out_graph=/home/aldopedraza/Documentos/ssd/extra_files/models/ssd_trans_model.pb \\\n",
    "--inputs='input_1' \\\n",
    "--outputs='predictions/concat' \\\n",
    "--transforms='\n",
    "  strip_unused_nodes(type=float, shape=\"1,512,512,3\")\n",
    "  remove_nodes(op=Identity, op=CheckNumerics)\n",
    "  fold_constants(ignore_errors=true)\n",
    "  fold_batch_norms\n",
    "  fold_old_batch_norms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m tensorflow.python.tools.optimize_for_inference \\\n",
    "--input=./extra_files/models/ssd_frozen_model.pb \\\n",
    "--output=./extra_files/models/ssd_opt_model.pb \\\n",
    "--frozen_graph=True --input_names=input_1 \\\n",
    "--output_names=predictions/concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary model\n",
    "# bazel build tensorflow/tools/graph_transforms:summarize_graph && \\\n",
    "# bazel-bin/tensorflow/tools/graph_transforms/summarize_graph \\\n",
    "# --in_graph=../ssd/extra_files/models/ssd_opt_model.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to tflite model\n",
    "Finally we conver the model to tflite format, we need a frozen model in the same way the input and output name are required, and all the operations in the model need to be conpatible with tflite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_def_file = \"/home/aldopedraza/Documentos/models/ssd_frozen_model.pb\"\n",
    "input_arrays = [\"input_1\"]\n",
    "output_arrays = [\"decoded_predictions/loop_over_batch/TensorArrayStack/TensorArrayGatherV3\"]\n",
    "\n",
    "converter = tf.contrib.lite.TocoConverter.from_frozen_graph(\n",
    "  graph_def_file, input_arrays, output_arrays, input_shapes={\"input_1\" : [1, 300, 300, 3]})\n",
    "tflite_model = converter.convert()\n",
    "open(\"/home/aldopedraza/Documentos/models/converted_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### THIS WORKS\n",
    "tflite_convert --output_file=/home/aldopedraza/Documentos/ssd/extra_files/models/tflite_model.tflite --graph_def_file=/home/aldopedraza/Documentos/ssd/extra_files/models/ssd_opt_model.pb --input_arrays=input_1 --output_arrays=predictions/concat --input_shapes=1,512,512,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !toco \\\n",
    "#   --graph_def_file=/home/aldopedraza/Documentos/ssd/extra_files/models/ssd_opt_model.pb \\\n",
    "#   --output_file=/home/aldopedraza/Documentos/ssd/extra_files/models/tflite_model.tflite \\\n",
    "#   --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\n",
    "#   --inference_type=QUANTIZED_UINT8 \\\n",
    "#   --input_shape=\"1,512,512,3\" \\\n",
    "#   --input_array=input_1 \\\n",
    "#   --output_array=predictions/concat \\\n",
    "#   --std_dev_values=127.5 --mean_value=127.5\n",
    "\n",
    "!toco \\\n",
    "--output_file=/home/aldopedraza/Documentos/ssd/extra_files/models/tflite_model.tflite \\\n",
    "--graph_def_file=/home/aldopedraza/Documentos/ssd/extra_files/models/ssd_opt_model.pb \\\n",
    "--output_format=TFLITE \\\n",
    "--inference_type=QUANTIZED_UINT8 \\\n",
    "--inference_input_type=FLOAT \\\n",
    "--input_arrays=input_1 \\\n",
    "--output_arrays=predictions/concat \\\n",
    "--input_shapes=1,512,512,3 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model to see if it's compatible with tflite\n",
    "\n",
    "In order to be sure that the model works correctly what we can do it's to use the interpreter to load the model and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aldopedraza/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/ipykernel/__main__.py:6: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/home/aldopedraza/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/ipykernel/__main__.py:7: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/home/aldopedraza/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/ipykernel/__main__.py:8: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9056888e-01 2.6890449e-04 2.9046461e-04 ... 1.0000000e-01\n",
      "  2.0000000e-01 2.0000000e-01]\n",
      " [9.9315298e-01 1.7266681e-04 2.5180369e-04 ... 1.0000000e-01\n",
      "  2.0000000e-01 2.0000000e-01]\n",
      " [9.9203098e-01 2.7664716e-04 2.3912858e-04 ... 1.0000000e-01\n",
      "  2.0000000e-01 2.0000000e-01]\n",
      " ...\n",
      " [9.9938989e-01 4.6993050e-06 3.1778568e-06 ... 1.0000000e-01\n",
      "  2.0000000e-01 2.0000000e-01]\n",
      " [9.9897754e-01 1.0331698e-05 5.4132288e-06 ... 1.0000000e-01\n",
      "  2.0000000e-01 2.0000000e-01]\n",
      " [9.9964786e-01 5.0593994e-06 2.8518134e-06 ... 1.0000000e-01\n",
      "  2.0000000e-01 2.0000000e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy import misc\n",
    "\n",
    "# Read image\n",
    "image_test = misc.imread('./image_test.jpg')\n",
    "image_test = misc.imresize(image_test, size=(300, 300))\n",
    "misc.imsave('./resize_image.jpg', image_test)\n",
    "image_test = image_test.reshape(1, 300, 300, 3)\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "# interpreter = tf.contrib.lite.Interpreter(model_path=\"/home/aldopedraza/Documentos/ssd/extra_files/models/converted_model.tflite\")\n",
    "interpreter = tf.contrib.lite.Interpreter(model_path=\"../models/converted_model.tflite\")\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(image_test, dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6810785  6639\n",
      "0.8500837  6753\n",
      "0.6522881  6867\n",
      "0.6811034  8223\n",
      "0.7581048  8283\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for value in output_data[0]:\n",
    "    if(value[15] > 0.5):\n",
    "        print(value[15], \" \" + str(count))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare cell phone output and computer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output from the cellphone is store in a string\n",
    "text2 = text.split(';')\n",
    "list1 = []\n",
    "for line in text.split(';'):\n",
    "    list2 = []\n",
    "    for j in line.split(','):\n",
    "        list2.append(float(j))\n",
    "    list1.append(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_computer = list(output_data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "com = np.array(output_computer)\n",
    "cell = np.array(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00565358"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell[6639][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6810784935951233"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com[0][6639][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015088855"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell[0][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11123182,  0.24621835, -3.7149367 , -3.3556237 ])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell[0][21:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01333333, 0.01333333, 0.1       , 0.1       ])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell[0][25: 29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.1, 0.2, 0.2])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell[0][29:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = 0\n",
    "i = 0\n",
    "index = 0\n",
    "for item in cell:\n",
    "    max_item = item.max()\n",
    "    if max_item > max_value:\n",
    "        max_value = max_item\n",
    "        index = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2823"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3140071604545618\n"
     ]
    }
   ],
   "source": [
    "distance_acc = 0\n",
    "for i in range(len(cell)):\n",
    "    distance_acc += distance.euclidean(list1[i], output_computer[0][i])\n",
    "    print(distance_acc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001504818094886122"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean error with euclidean distance\n",
    "distance_acc / 8732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052321661143207895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "error = 0\n",
    "for i in range(len(cell)):\n",
    "    error += mean_squared_error(list1[i], output_computer[0][i])\n",
    "    print(error)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.99194470261199e-06"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean error with mean square error\n",
    "error / 8732"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
