{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from extra_files import helper as hp\n",
    "from imageio import imwrite, imread\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "data_path = '/home/aldo/Documents/data-cic/'\n",
    "preprocess_path = data_path + 'preprocess_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SSDLite300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/aldo/Documents/ssd/data_generator/object_detection_2d_data_generator.py:43: UserWarning: 'BeautifulSoup' module is missing. The XML-parser will be unavailable.\n",
      "  warnings.warn(\"'BeautifulSoup' module is missing. The XML-parser will be unavailable.\")\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssdlite320_mobilenetv2_conv13_17 import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "from extra_files.f1_callback import F1_callback as f1_call\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters (original SSD300 architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameteres needed for ssd_300() and SSDInputEncoder()\n",
    "\n",
    "img_height = 320 # Height of the model input images\n",
    "img_width = 320 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [-1., -1., -1.] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "divide_by_stddev = [127.5, 127.5, 127.5]\n",
    "swap_channels = False # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 1 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "# new_scales = [0.15, 0.33, 0.47, 0.61, 0.76, 0.90, 1.05]\n",
    "scales = [0.15, 0.55, 1.05]\n",
    "aspect_ratios = [[1.0, 0.5, 2.0/3.0],\n",
    "                 [1.0, 0.5, 2.0/3.0, 2.5/3.0, 1.0/3.0, 3.0/4.0]]\n",
    "# aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "#                  [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "#                  [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "#                  [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "#                  [1.0, 2.0, 0.5],\n",
    "#                  [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [16, 32] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new model with SSD weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Build the Keras model.\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=mean_color,\n",
    "                divide_by_stddev=divide_by_stddev,\n",
    "                swap_channels=swap_channels)\n",
    "\n",
    "# 2: Load some weights into the model.\n",
    "\n",
    "# TODO: Set the path to the weights you want to load.\n",
    "# weights_path = '../weights/models/ssdlite320_mobilenetv1/ssdlite320_mobilenetv1_pascal.h5'\n",
    "# model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "# 3: Instantiate an optimizer and the SSD loss function and compile the model.\n",
    "#    If you want to follow the original Caffe implementation, use the preset SGD\n",
    "#    optimizer, otherwise I'd recommend the commented-out Adam optimizer.\n",
    "\n",
    "adam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 320, 320, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "identity_layer (Lambda)         (None, 320, 320, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_stddev_normalization (Lam (None, 320, 320, 3)  0           identity_layer[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_mean_normalization (Lambd (None, 320, 320, 3)  0           input_stddev_normalization[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 multiple             558656      input_mean_normalization[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "mobl13_conv_expand (Conv2D)     (None, 20, 20, 576)  55296       model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn13_conv_bn_expand (BatchNorma (None, 20, 20, 576)  2304        mobl13_conv_expand[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_13_relu (Activation)       (None, 20, 20, 576)  0           bn13_conv_bn_expand[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "mobl13_conv_depthwise (Depthwis (None, 20, 20, 576)  5184        conv_13_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn13_conv_depthwise (BatchNorma (None, 20, 20, 576)  2304        mobl13_conv_depthwise[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_13_relu (Activation)    (None, 20, 20, 576)  0           bn13_conv_depthwise[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "mobl13_conv_project (Conv2D)    (None, 20, 20, 160)  92160       conv_dw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn13_conv_bn_project (BatchNorm (None, 20, 20, 160)  640         mobl13_conv_project[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "mobl14_conv_expand (Conv2D)     (None, 20, 20, 960)  153600      bn13_conv_bn_project[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn14_conv_bn_expand (BatchNorma (None, 20, 20, 960)  3840        mobl14_conv_expand[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_14_relu (Activation)       (None, 20, 20, 960)  0           bn14_conv_bn_expand[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "mobl14_conv_depthwise (Depthwis (None, 20, 20, 960)  8640        conv_14_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn14_conv_depthwise (BatchNorma (None, 20, 20, 960)  3840        mobl14_conv_depthwise[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_14_relu (Activation)    (None, 20, 20, 960)  0           bn14_conv_depthwise[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "mobl14_conv_project (Conv2D)    (None, 20, 20, 160)  153600      conv_dw_14_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn14_conv_bn_project (BatchNorm (None, 20, 20, 160)  640         mobl14_conv_project[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res_connect_14 (Add)            (None, 20, 20, 160)  0           bn13_conv_bn_project[0][0]       \n",
      "                                                                 bn14_conv_bn_project[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "mobl15_conv_expand (Conv2D)     (None, 20, 20, 960)  153600      res_connect_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn15_conv_bn_expand (BatchNorma (None, 20, 20, 960)  3840        mobl15_conv_expand[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_15_relu (Activation)       (None, 20, 20, 960)  0           bn15_conv_bn_expand[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "mobl15_conv_depthwise (Depthwis (None, 20, 20, 960)  8640        conv_15_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn15_conv_depthwise (BatchNorma (None, 20, 20, 960)  3840        mobl15_conv_depthwise[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_15_relu (Activation)    (None, 20, 20, 960)  0           bn15_conv_depthwise[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "mobl15_conv_project (Conv2D)    (None, 20, 20, 160)  153600      conv_dw_15_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn15_conv_bn_project (BatchNorm (None, 20, 20, 160)  640         mobl15_conv_project[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res_connect_15 (Add)            (None, 20, 20, 160)  0           res_connect_14[0][0]             \n",
      "                                                                 bn15_conv_bn_project[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "mobl16_conv_expand (Conv2D)     (None, 20, 20, 960)  153600      res_connect_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn16_conv_bn_expand (BatchNorma (None, 20, 20, 960)  3840        mobl16_conv_expand[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_16_relu (Activation)       (None, 20, 20, 960)  0           bn16_conv_bn_expand[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "mobl16_conv_depthwise (Depthwis (None, 20, 20, 960)  8640        conv_16_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn16_conv_depthwise (BatchNorma (None, 20, 20, 960)  3840        mobl16_conv_depthwise[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_16_relu (Activation)    (None, 20, 20, 960)  0           bn16_conv_depthwise[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "mobl16_conv_project (Conv2D)    (None, 20, 20, 320)  307200      conv_dw_16_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn16_conv_bn_project (BatchNorm (None, 20, 20, 320)  1280        mobl16_conv_project[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 10, 10, 1280) 409600      bn16_conv_bn_project[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv13_mbox_conf (Conv2D)       (None, 20, 20, 8)    41480       mobl13_conv_expand[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv17_mbox_conf (Conv2D)       (None, 10, 10, 14)   161294      Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv13_mbox_loc (Conv2D)        (None, 20, 20, 16)   82960       mobl13_conv_expand[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv17_mbox_loc (Conv2D)        (None, 10, 10, 28)   322588      Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv11_mbox_conf_reshape (Resha (None, 1600, 2)      0           conv13_mbox_conf[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv13_mbox_conf_reshape (Resha (None, 700, 2)       0           conv17_mbox_conf[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv13_mbox_priorbox (AnchorBox (None, 20, 20, 4, 8) 0           conv13_mbox_loc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv17_mbox_priorbox (AnchorBox (None, 10, 10, 7, 8) 0           conv17_mbox_loc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mbox_conf (Concatenate)         (None, 2300, 2)      0           conv11_mbox_conf_reshape[0][0]   \n",
      "                                                                 conv13_mbox_conf_reshape[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv11_mbox_loc_reshape (Reshap (None, 1600, 4)      0           conv13_mbox_loc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv13_mbox_loc_reshape (Reshap (None, 700, 4)       0           conv17_mbox_loc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv13_mbox_priorbox_reshape (R (None, 1600, 8)      0           conv13_mbox_priorbox[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv17_mbox_priorbox_reshape (R (None, 700, 8)       0           conv17_mbox_priorbox[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "mbox_conf_softmax (Activation)  (None, 2300, 2)      0           mbox_conf[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mbox_loc (Concatenate)          (None, 2300, 4)      0           conv11_mbox_loc_reshape[0][0]    \n",
      "                                                                 conv13_mbox_loc_reshape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mbox_priorbox (Concatenate)     (None, 2300, 8)      0           conv13_mbox_priorbox_reshape[0][0\n",
      "                                                                 conv17_mbox_priorbox_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, 2300, 14)     0           mbox_conf_softmax[0][0]          \n",
      "                                                                 mbox_loc[0][0]                   \n",
      "                                                                 mbox_priorbox[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,861,186\n",
      "Trainable params: 2,829,634\n",
      "Non-trainable params: 31,552\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images into memory: 100%|██████████| 6469/6469 [00:27<00:00, 237.93it/s]\n",
      "Loading images into memory: 100%|██████████| 2097/2097 [00:08<00:00, 236.63it/s]\n",
      "Number of images in the training dataset:\t  6469\n",
      "Number of images in the validation dataset:\t  2097\n"
     ]
    }
   ],
   "source": [
    "# 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
    "\n",
    "# Optional: If you have enough memory, consider loading the images into memory for the reasons explained above.\n",
    "\n",
    "train_dataset = DataGenerator(load_images_into_memory=True, hdf5_dataset_path=None)\n",
    "val_dataset = DataGenerator(load_images_into_memory=True, hdf5_dataset_path=None)\n",
    "\n",
    "# 2: Parse the image and label lists for the training and validation datasets.\n",
    "\n",
    "# TODO: Set the paths to your dataset here.\n",
    "\n",
    "# Images\n",
    "# images_dir = data_path + 'udacity_driving_datasets'\n",
    "images_dir = data_path + 'pascal_dataset'\n",
    "# images_dir = data_path + 'images'\n",
    "\n",
    "# Ground truth\n",
    "# train_labels_filename = preprocess_path + '/udc.csv'\n",
    "# train_labels_filename = preprocess_path + '/cic_pascal_train.csv'\n",
    "# val_labels_filename   = preprocess_path + '/cic_pascal_val.csv'\n",
    "train_labels_filename = preprocess_path + '/pascal_train.csv'\n",
    "val_labels_filename   = preprocess_path + '/pascal_val.csv'\n",
    "# train_labels_filename = preprocess_path + '/cic_train.csv'\n",
    "# val_labels_filename   = preprocess_path + '/cic_val.csv'\n",
    "\n",
    "train_dataset.parse_csv(images_dir=images_dir,\n",
    "                        labels_filename=train_labels_filename,\n",
    "                        input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'], # This is the order of the first six columns in the CSV file that contains the labels for your dataset. If your labels are in XML format, maybe the XML parser will be helpful, check the documentation.\n",
    "                        include_classes='all')\n",
    "\n",
    "val_dataset.parse_csv(images_dir=images_dir,\n",
    "                      labels_filename=val_labels_filename,\n",
    "                      input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'],\n",
    "                      include_classes='all')\n",
    "\n",
    "# Optional: Convert the dataset into an HDF5 dataset. This will require more disk space, but will\n",
    "# speed up the training. Doing this is not relevant in case you activated the `load_images_into_memory`\n",
    "# option in the constructor, because in that cas the images are in memory already anyway. If you don't\n",
    "# want to create HDF5 datasets, comment out the subsequent two function calls.\n",
    "\n",
    "# train_dataset.create_hdf5_dataset(file_path='dataset_udacity_traffic_train.h5',\n",
    "#                                   resize=False,\n",
    "#                                   variable_image_size=True,\n",
    "#                                   verbose=True)\n",
    "\n",
    "# val_dataset.create_hdf5_dataset(file_path='dataset_udacity_traffic_val.h5',\n",
    "#                                 resize=False,\n",
    "#                                 variable_image_size=True,\n",
    "#                                 verbose=True)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training dataset:\t  6469\n",
      "Number of images in the validation dataset:\t  2097\n"
     ]
    }
   ],
   "source": [
    "# 3: Set the batch size.\n",
    "batch_size = 30 # Change the batch size if you like, or if you run into GPU memory issues.\n",
    "\n",
    "# 4: Set the image transformations for pre-processing and data augmentation options.\n",
    "# For the training generator:\n",
    "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
    "                                            img_width=img_width,\n",
    "                                            background=mean_color)\n",
    "\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [model.get_layer('conv13_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv17_mbox_conf').output_shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[ssd_data_augmentation],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_val = np.load('../data-cic/preprocess_data/label_val_320.npy')\n",
    "val_images_320 = np.load('../data-cic/preprocess_data/images_val_320x320.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model callbacks.\n",
    "\n",
    "# TODO: Set the filepath under which you want to save the model.\n",
    "model_checkpoint = ModelCheckpoint(filepath='/home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "\n",
    "csv_logger = CSVLogger(filename=data_path + 'history/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0.0,\n",
    "                               patience=12,\n",
    "                               verbose=1)\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                         factor=0.5,\n",
    "                                         patience=10,\n",
    "                                         verbose=1,\n",
    "                                         min_delta=0.001,\n",
    "                                         cooldown=0,\n",
    "                                         min_lr=0.000001)\n",
    "\n",
    "f1_callback = f1_call(0.20, \n",
    "                      0.45, \n",
    "                      200, \n",
    "                      normalize_coords, \n",
    "                      img_height, \n",
    "                      img_width, \n",
    "                      val_images_320, \n",
    "                      label_val, (1, 2300, 14),\n",
    "                      data_path + 'history/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_f1_13_17.csv')\n",
    "\n",
    "# learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule, verbose=1)\n",
    "\n",
    "\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             early_stopping,\n",
    "#              learning_rate_scheduler]\n",
    "             reduce_learning_rate,\n",
    "             f1_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "420/420 [==============================] - 236s 561ms/step - loss: 4.6646 - val_loss: 5.3254\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.32537, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.237\n",
      "Recall: 0.1936\n",
      "F1 score: 0.2131\n",
      "Improve F1 score from -inf to 0.21312238585304102\n",
      "Epoch 2/1000\n",
      "420/420 [==============================] - 220s 524ms/step - loss: 3.5603 - val_loss: 4.9073\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.32537 to 4.90725, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.2338\n",
      "Recall: 0.3023\n",
      "F1 score: 0.2637\n",
      "Improve F1 score from 0.21312238585304102 to 0.2636890033682092\n",
      "Epoch 3/1000\n",
      "420/420 [==============================] - 220s 524ms/step - loss: 3.3258 - val_loss: 5.1591\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 4.90725\n",
      "Number of images: 45\n",
      "Presicion: 0.2494\n",
      "Recall: 0.3747\n",
      "F1 score: 0.2995\n",
      "Improve F1 score from 0.2636890033682092 to 0.2994566553723911\n",
      "Epoch 4/1000\n",
      "420/420 [==============================] - 219s 522ms/step - loss: 3.5704 - val_loss: 6.5213\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.90725\n",
      "Number of images: 45\n",
      "Presicion: 0.0686\n",
      "Recall: 0.4711\n",
      "F1 score: 0.1198\n",
      "Epoch 5/1000\n",
      "420/420 [==============================] - 220s 524ms/step - loss: 3.2204 - val_loss: 4.7076\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.90725 to 4.70759, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.0532\n",
      "Recall: 0.4794\n",
      "F1 score: 0.0958\n",
      "Epoch 6/1000\n",
      "420/420 [==============================] - 219s 522ms/step - loss: 3.1089 - val_loss: 4.0093\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.70759 to 4.00931, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.0814\n",
      "Recall: 0.4884\n",
      "F1 score: 0.1395\n",
      "Epoch 7/1000\n",
      "420/420 [==============================] - 222s 528ms/step - loss: 3.0169 - val_loss: 4.1868\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4.00931\n",
      "Number of images: 45\n",
      "Presicion: 0.0798\n",
      "Recall: 0.4621\n",
      "F1 score: 0.1361\n",
      "Epoch 8/1000\n",
      "420/420 [==============================] - 218s 518ms/step - loss: 3.0135 - val_loss: 3.4282\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.00931 to 3.42823, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.3332\n",
      "Recall: 0.4004\n",
      "F1 score: 0.3637\n",
      "Improve F1 score from 0.2994566553723911 to 0.36373817574733885\n",
      "Epoch 9/1000\n",
      "420/420 [==============================] - 220s 524ms/step - loss: 2.9483 - val_loss: 3.6538\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3.42823\n",
      "Number of images: 45\n",
      "Presicion: 0.3976\n",
      "Recall: 0.266\n",
      "F1 score: 0.3188\n",
      "Epoch 10/1000\n",
      "420/420 [==============================] - 220s 523ms/step - loss: 2.9067 - val_loss: 3.8105\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.42823\n",
      "Number of images: 45\n",
      "Presicion: 0.3035\n",
      "Recall: 0.2584\n",
      "F1 score: 0.2791\n",
      "Epoch 11/1000\n",
      "420/420 [==============================] - 219s 521ms/step - loss: 2.8896 - val_loss: 3.3273\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.42823 to 3.32726, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.3472\n",
      "Recall: 0.3168\n",
      "F1 score: 0.3313\n",
      "Epoch 12/1000\n",
      "420/420 [==============================] - 221s 526ms/step - loss: 2.8639 - val_loss: 3.2552\n",
      "\n",
      "Epoch 00012: val_loss improved from 3.32726 to 3.25517, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.4229\n",
      "Recall: 0.3361\n",
      "F1 score: 0.3746\n",
      "Improve F1 score from 0.36373817574733885 to 0.3745646996285553\n",
      "Epoch 13/1000\n",
      "420/420 [==============================] - 218s 519ms/step - loss: 2.8435 - val_loss: 3.2051\n",
      "\n",
      "Epoch 00013: val_loss improved from 3.25517 to 3.20510, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.4128\n",
      "Recall: 0.287\n",
      "F1 score: 0.3386\n",
      "Epoch 14/1000\n",
      "420/420 [==============================] - 220s 524ms/step - loss: 2.8252 - val_loss: 3.0345\n",
      "\n",
      "Epoch 00014: val_loss improved from 3.20510 to 3.03449, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.4553\n",
      "Recall: 0.2959\n",
      "F1 score: 0.3587\n",
      "Epoch 15/1000\n",
      "420/420 [==============================] - 220s 525ms/step - loss: 2.7953 - val_loss: 3.0448\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 3.03449\n",
      "Number of images: 45\n",
      "Presicion: 0.4721\n",
      "Recall: 0.3621\n",
      "F1 score: 0.4098\n",
      "Improve F1 score from 0.3745646996285553 to 0.4098321746334187\n",
      "Epoch 16/1000\n",
      "420/420 [==============================] - 220s 523ms/step - loss: 2.7642 - val_loss: 3.2052\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 3.03449\n",
      "Number of images: 45\n",
      "Presicion: 0.3694\n",
      "Recall: 0.4425\n",
      "F1 score: 0.4027\n",
      "Epoch 17/1000\n",
      "420/420 [==============================] - 219s 521ms/step - loss: 2.7467 - val_loss: 3.6967\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 3.03449\n",
      "Number of images: 45\n",
      "Presicion: 0.4218\n",
      "Recall: 0.3067\n",
      "F1 score: 0.3552\n",
      "Epoch 18/1000\n",
      "420/420 [==============================] - 220s 523ms/step - loss: 2.7479 - val_loss: 3.0190\n",
      "\n",
      "Epoch 00018: val_loss improved from 3.03449 to 3.01900, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.3441\n",
      "Recall: 0.2816\n",
      "F1 score: 0.3097\n",
      "Epoch 19/1000\n",
      "420/420 [==============================] - 223s 530ms/step - loss: 2.7412 - val_loss: 2.8495\n",
      "\n",
      "Epoch 00019: val_loss improved from 3.01900 to 2.84953, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.4774\n",
      "Recall: 0.3166\n",
      "F1 score: 0.3807\n",
      "Epoch 20/1000\n",
      "420/420 [==============================] - 219s 522ms/step - loss: 2.6945 - val_loss: 3.0234\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.84953\n",
      "Number of images: 45\n",
      "Presicion: 0.4145\n",
      "Recall: 0.3151\n",
      "F1 score: 0.3581\n",
      "Epoch 21/1000\n",
      "420/420 [==============================] - 220s 523ms/step - loss: 2.7095 - val_loss: 2.9107\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.84953\n",
      "Number of images: 45\n",
      "Presicion: 0.5117\n",
      "Recall: 0.3488\n",
      "F1 score: 0.4148\n",
      "Improve F1 score from 0.4098321746334187 to 0.41480958852107336\n",
      "Epoch 22/1000\n",
      "420/420 [==============================] - 221s 526ms/step - loss: 2.6799 - val_loss: 2.9429\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.84953\n",
      "Number of images: 45\n",
      "Presicion: 0.4495\n",
      "Recall: 0.2689\n",
      "F1 score: 0.3365\n",
      "Epoch 23/1000\n",
      "420/420 [==============================] - 219s 521ms/step - loss: 2.6626 - val_loss: 2.9877\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.84953\n",
      "Number of images: 45\n",
      "Presicion: 0.3582\n",
      "Recall: 0.3875\n",
      "F1 score: 0.3723\n",
      "Epoch 24/1000\n",
      "420/420 [==============================] - 220s 524ms/step - loss: 2.6472 - val_loss: 2.9210\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.84953\n",
      "Number of images: 45\n",
      "Presicion: 0.43\n",
      "Recall: 0.4624\n",
      "F1 score: 0.4456\n",
      "Improve F1 score from 0.41480958852107336 to 0.44561637340315396\n",
      "Epoch 25/1000\n",
      "420/420 [==============================] - 221s 527ms/step - loss: 2.6532 - val_loss: 2.9680\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.84953\n",
      "Number of images: 45\n",
      "Presicion: 0.4207\n",
      "Recall: 0.3645\n",
      "F1 score: 0.3906\n",
      "Epoch 26/1000\n",
      "420/420 [==============================] - 221s 526ms/step - loss: 2.5944 - val_loss: 3.3147\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.84953\n",
      "Number of images: 45\n",
      "Presicion: 0.4764\n",
      "Recall: 0.3379\n",
      "F1 score: 0.3954\n",
      "Epoch 27/1000\n",
      "420/420 [==============================] - 220s 524ms/step - loss: 2.6030 - val_loss: 2.7001\n",
      "\n",
      "Epoch 00027: val_loss improved from 2.84953 to 2.70012, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.488\n",
      "Recall: 0.3485\n",
      "F1 score: 0.4066\n",
      "Epoch 28/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/420 [==============================] - 219s 521ms/step - loss: 2.6020 - val_loss: 2.8236\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.70012\n",
      "Number of images: 45\n",
      "Presicion: 0.4333\n",
      "Recall: 0.2978\n",
      "F1 score: 0.353\n",
      "Epoch 29/1000\n",
      "420/420 [==============================] - 219s 522ms/step - loss: 2.6227 - val_loss: 2.9477\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.70012\n",
      "Number of images: 45\n",
      "Presicion: 0.489\n",
      "Recall: 0.2863\n",
      "F1 score: 0.3612\n",
      "Epoch 30/1000\n",
      "420/420 [==============================] - 219s 521ms/step - loss: 2.5624 - val_loss: 2.8319\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.70012\n",
      "Number of images: 45\n",
      "Presicion: 0.5194\n",
      "Recall: 0.3407\n",
      "F1 score: 0.4115\n",
      "Epoch 31/1000\n",
      "420/420 [==============================] - 218s 520ms/step - loss: 2.5574 - val_loss: 3.0575\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.70012\n",
      "Number of images: 45\n",
      "Presicion: 0.4825\n",
      "Recall: 0.2539\n",
      "F1 score: 0.3327\n",
      "Epoch 32/1000\n",
      "420/420 [==============================] - 219s 521ms/step - loss: 2.5344 - val_loss: 2.9309\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2.70012\n",
      "Number of images: 45\n",
      "Presicion: 0.5334\n",
      "Recall: 0.2978\n",
      "F1 score: 0.3822\n",
      "Epoch 33/1000\n",
      "420/420 [==============================] - 221s 527ms/step - loss: 2.5538 - val_loss: 2.9022\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2.70012\n",
      "Number of images: 45\n",
      "Presicion: 0.5383\n",
      "Recall: 0.2971\n",
      "F1 score: 0.3828\n",
      "Epoch 34/1000\n",
      "420/420 [==============================] - 219s 522ms/step - loss: 2.5423 - val_loss: 3.0160\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2.70012\n",
      "Number of images: 45\n",
      "Presicion: 0.3857\n",
      "Recall: 0.2381\n",
      "F1 score: 0.2944\n",
      "Epoch 35/1000\n",
      "420/420 [==============================] - 218s 519ms/step - loss: 2.6026 - val_loss: 2.9730\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 2.70012\n",
      "Number of images: 45\n",
      "Presicion: 0.5199\n",
      "Recall: 0.1454\n",
      "F1 score: 0.2273\n",
      "Epoch 36/1000\n",
      "420/420 [==============================] - 221s 526ms/step - loss: 2.5025 - val_loss: 2.7298\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 2.70012\n",
      "Number of images: 45\n",
      "Presicion: 0.4955\n",
      "Recall: 0.2178\n",
      "F1 score: 0.3026\n",
      "Epoch 37/1000\n",
      "420/420 [==============================] - 217s 518ms/step - loss: 2.5184 - val_loss: 2.6363\n",
      "\n",
      "Epoch 00037: val_loss improved from 2.70012 to 2.63632, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.5273\n",
      "Recall: 0.2905\n",
      "F1 score: 0.3746\n",
      "Epoch 38/1000\n",
      "420/420 [==============================] - 219s 521ms/step - loss: 2.4847 - val_loss: 2.8324\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2.63632\n",
      "Number of images: 45\n",
      "Presicion: 0.5312\n",
      "Recall: 0.2484\n",
      "F1 score: 0.3385\n",
      "Epoch 39/1000\n",
      "420/420 [==============================] - 219s 520ms/step - loss: 2.4633 - val_loss: 2.6874\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 2.63632\n",
      "Number of images: 45\n",
      "Presicion: 0.5476\n",
      "Recall: 0.2835\n",
      "F1 score: 0.3736\n",
      "Epoch 40/1000\n",
      "420/420 [==============================] - 220s 523ms/step - loss: 2.4952 - val_loss: 2.6668\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 2.63632\n",
      "Number of images: 45\n",
      "Presicion: 0.5408\n",
      "Recall: 0.321\n",
      "F1 score: 0.4028\n",
      "Epoch 41/1000\n",
      "420/420 [==============================] - 228s 543ms/step - loss: 2.4407 - val_loss: 2.6673\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2.63632\n",
      "Number of images: 45\n",
      "Presicion: 0.5522\n",
      "Recall: 0.3506\n",
      "F1 score: 0.4288\n",
      "Epoch 42/1000\n",
      "420/420 [==============================] - 233s 554ms/step - loss: 2.4663 - val_loss: 2.6285\n",
      "\n",
      "Epoch 00042: val_loss improved from 2.63632 to 2.62848, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.5785\n",
      "Recall: 0.3921\n",
      "F1 score: 0.4674\n",
      "Improve F1 score from 0.44561637340315396 to 0.4674387216883854\n",
      "Epoch 43/1000\n",
      "420/420 [==============================] - 228s 542ms/step - loss: 2.4339 - val_loss: 2.7287\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2.62848\n",
      "Number of images: 45\n",
      "Presicion: 0.5892\n",
      "Recall: 0.3045\n",
      "F1 score: 0.4015\n",
      "Epoch 44/1000\n",
      "420/420 [==============================] - 222s 527ms/step - loss: 2.4268 - val_loss: 2.6704\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.62848\n",
      "Number of images: 45\n",
      "Presicion: 0.5841\n",
      "Recall: 0.3316\n",
      "F1 score: 0.4231\n",
      "Epoch 45/1000\n",
      "420/420 [==============================] - 218s 519ms/step - loss: 2.4363 - val_loss: 2.6790\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.62848\n",
      "Number of images: 45\n",
      "Presicion: 0.5737\n",
      "Recall: 0.2419\n",
      "F1 score: 0.3403\n",
      "Epoch 46/1000\n",
      "420/420 [==============================] - 220s 524ms/step - loss: 2.4272 - val_loss: 2.6110\n",
      "\n",
      "Epoch 00046: val_loss improved from 2.62848 to 2.61102, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.5491\n",
      "Recall: 0.2689\n",
      "F1 score: 0.361\n",
      "Epoch 47/1000\n",
      "420/420 [==============================] - 220s 523ms/step - loss: 2.4326 - val_loss: 2.8943\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2.61102\n",
      "Number of images: 45\n",
      "Presicion: 0.584\n",
      "Recall: 0.2121\n",
      "F1 score: 0.3111\n",
      "Epoch 48/1000\n",
      "420/420 [==============================] - 222s 528ms/step - loss: 2.4317 - val_loss: 2.8547\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.61102\n",
      "Number of images: 45\n",
      "Presicion: 0.5291\n",
      "Recall: 0.2798\n",
      "F1 score: 0.366\n",
      "Epoch 49/1000\n",
      "420/420 [==============================] - 219s 522ms/step - loss: 2.4040 - val_loss: 2.6249\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.61102\n",
      "Number of images: 45\n",
      "Presicion: 0.5615\n",
      "Recall: 0.3077\n",
      "F1 score: 0.3975\n",
      "Epoch 50/1000\n",
      "420/420 [==============================] - 219s 523ms/step - loss: 2.4014 - val_loss: 2.6277\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.61102\n",
      "Number of images: 45\n",
      "Presicion: 0.5156\n",
      "Recall: 0.3115\n",
      "F1 score: 0.3884\n",
      "Epoch 51/1000\n",
      "420/420 [==============================] - 221s 527ms/step - loss: 2.4444 - val_loss: 2.6390\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.61102\n",
      "Number of images: 45\n",
      "Presicion: 0.5235\n",
      "Recall: 0.3571\n",
      "F1 score: 0.4246\n",
      "Epoch 52/1000\n",
      "420/420 [==============================] - 218s 520ms/step - loss: 2.3902 - val_loss: 3.0894\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2.61102\n",
      "Number of images: 45\n",
      "Presicion: 0.4903\n",
      "Recall: 0.2873\n",
      "F1 score: 0.3623\n",
      "Epoch 53/1000\n",
      "420/420 [==============================] - 221s 527ms/step - loss: 2.3781 - val_loss: 2.7133\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.61102\n",
      "Number of images: 45\n",
      "Presicion: 0.5089\n",
      "Recall: 0.2949\n",
      "F1 score: 0.3734\n",
      "Epoch 54/1000\n",
      "420/420 [==============================] - 220s 525ms/step - loss: 2.3773 - val_loss: 2.7362\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.61102\n",
      "Number of images: 45\n",
      "Presicion: 0.5228\n",
      "Recall: 0.2696\n",
      "F1 score: 0.3558\n",
      "Epoch 55/1000\n",
      "420/420 [==============================] - 220s 524ms/step - loss: 2.3743 - val_loss: 2.7362\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 2.61102\n",
      "Number of images: 45\n",
      "Presicion: 0.5538\n",
      "Recall: 0.3446\n",
      "F1 score: 0.4248\n",
      "Epoch 56/1000\n",
      "420/420 [==============================] - 221s 526ms/step - loss: 2.3604 - val_loss: 2.6237\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 2.61102\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Number of images: 45\n",
      "Presicion: 0.5259\n",
      "Recall: 0.2875\n",
      "F1 score: 0.3718\n",
      "Epoch 57/1000\n",
      "420/420 [==============================] - 220s 524ms/step - loss: 2.2704 - val_loss: 2.4578\n",
      "\n",
      "Epoch 00057: val_loss improved from 2.61102 to 2.45780, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.551\n",
      "Recall: 0.2694\n",
      "F1 score: 0.3619\n",
      "Epoch 58/1000\n",
      "420/420 [==============================] - 220s 524ms/step - loss: 2.2108 - val_loss: 2.4444\n",
      "\n",
      "Epoch 00058: val_loss improved from 2.45780 to 2.44445, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.5874\n",
      "Recall: 0.2447\n",
      "F1 score: 0.3455\n",
      "Epoch 59/1000\n",
      "420/420 [==============================] - 220s 523ms/step - loss: 2.1950 - val_loss: 2.4283\n",
      "\n",
      "Epoch 00059: val_loss improved from 2.44445 to 2.42826, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.5605\n",
      "Recall: 0.3063\n",
      "F1 score: 0.3962\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/420 [==============================] - 220s 523ms/step - loss: 2.2125 - val_loss: 2.4232\n",
      "\n",
      "Epoch 00060: val_loss improved from 2.42826 to 2.42317, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.5835\n",
      "Recall: 0.3037\n",
      "F1 score: 0.3995\n",
      "Epoch 61/1000\n",
      "420/420 [==============================] - 218s 519ms/step - loss: 2.1809 - val_loss: 2.3699\n",
      "\n",
      "Epoch 00061: val_loss improved from 2.42317 to 2.36991, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.5911\n",
      "Recall: 0.3102\n",
      "F1 score: 0.4069\n",
      "Epoch 62/1000\n",
      "420/420 [==============================] - 220s 523ms/step - loss: 2.1981 - val_loss: 2.4524\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 2.36991\n",
      "Number of images: 45\n",
      "Presicion: 0.601\n",
      "Recall: 0.3067\n",
      "F1 score: 0.4061\n",
      "Epoch 63/1000\n",
      "420/420 [==============================] - 219s 522ms/step - loss: 2.1981 - val_loss: 2.3685\n",
      "\n",
      "Epoch 00063: val_loss improved from 2.36991 to 2.36851, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.6014\n",
      "Recall: 0.301\n",
      "F1 score: 0.4012\n",
      "Epoch 64/1000\n",
      "420/420 [==============================] - 221s 525ms/step - loss: 2.1594 - val_loss: 2.4463\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 2.36851\n",
      "Number of images: 45\n",
      "Presicion: 0.5577\n",
      "Recall: 0.3083\n",
      "F1 score: 0.3971\n",
      "Epoch 65/1000\n",
      "420/420 [==============================] - 218s 520ms/step - loss: 2.1381 - val_loss: 2.4573\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 2.36851\n",
      "Number of images: 45\n",
      "Presicion: 0.604\n",
      "Recall: 0.3001\n",
      "F1 score: 0.401\n",
      "Epoch 66/1000\n",
      "420/420 [==============================] - 218s 519ms/step - loss: 2.1710 - val_loss: 2.4257\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 2.36851\n",
      "Number of images: 45\n",
      "Presicion: 0.5716\n",
      "Recall: 0.2799\n",
      "F1 score: 0.3758\n",
      "Epoch 67/1000\n",
      "420/420 [==============================] - 219s 521ms/step - loss: 2.1826 - val_loss: 2.3780\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 2.36851\n",
      "Number of images: 45\n",
      "Presicion: 0.5701\n",
      "Recall: 0.339\n",
      "F1 score: 0.4252\n",
      "Epoch 68/1000\n",
      "420/420 [==============================] - 220s 523ms/step - loss: 2.1743 - val_loss: 2.3891\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 2.36851\n",
      "Number of images: 45\n",
      "Presicion: 0.5899\n",
      "Recall: 0.3401\n",
      "F1 score: 0.4315\n",
      "Epoch 69/1000\n",
      "420/420 [==============================] - 218s 519ms/step - loss: 2.0945 - val_loss: 2.3866\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2.36851\n",
      "Number of images: 45\n",
      "Presicion: 0.608\n",
      "Recall: 0.3326\n",
      "F1 score: 0.43\n",
      "Epoch 70/1000\n",
      "420/420 [==============================] - 219s 520ms/step - loss: 2.1517 - val_loss: 2.4215\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 2.36851\n",
      "Number of images: 45\n",
      "Presicion: 0.592\n",
      "Recall: 0.3224\n",
      "F1 score: 0.4175\n",
      "Epoch 71/1000\n",
      "420/420 [==============================] - 219s 522ms/step - loss: 2.1289 - val_loss: 2.3202\n",
      "\n",
      "Epoch 00071: val_loss improved from 2.36851 to 2.32016, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.6035\n",
      "Recall: 0.2978\n",
      "F1 score: 0.3988\n",
      "Epoch 72/1000\n",
      "420/420 [==============================] - 217s 517ms/step - loss: 2.1190 - val_loss: 2.2961\n",
      "\n",
      "Epoch 00072: val_loss improved from 2.32016 to 2.29606, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.6047\n",
      "Recall: 0.3228\n",
      "F1 score: 0.4209\n",
      "Epoch 73/1000\n",
      "420/420 [==============================] - 218s 520ms/step - loss: 2.1249 - val_loss: 2.3942\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 2.29606\n",
      "Number of images: 45\n",
      "Presicion: 0.6015\n",
      "Recall: 0.3153\n",
      "F1 score: 0.4137\n",
      "Epoch 74/1000\n",
      "420/420 [==============================] - 220s 524ms/step - loss: 2.1269 - val_loss: 2.4554\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 2.29606\n",
      "Number of images: 45\n",
      "Presicion: 0.5801\n",
      "Recall: 0.3574\n",
      "F1 score: 0.4423\n",
      "Epoch 75/1000\n",
      "420/420 [==============================] - 218s 520ms/step - loss: 2.1218 - val_loss: 2.3761\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 2.29606\n",
      "Number of images: 45\n",
      "Presicion: 0.6001\n",
      "Recall: 0.285\n",
      "F1 score: 0.3864\n",
      "Epoch 76/1000\n",
      "420/420 [==============================] - 217s 517ms/step - loss: 2.1076 - val_loss: 2.3997\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 2.29606\n",
      "Number of images: 45\n",
      "Presicion: 0.5958\n",
      "Recall: 0.2778\n",
      "F1 score: 0.3789\n",
      "Epoch 77/1000\n",
      "420/420 [==============================] - 220s 524ms/step - loss: 2.1274 - val_loss: 2.3680\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 2.29606\n",
      "Number of images: 45\n",
      "Presicion: 0.5816\n",
      "Recall: 0.3375\n",
      "F1 score: 0.4272\n",
      "Epoch 78/1000\n",
      "420/420 [==============================] - 219s 522ms/step - loss: 2.1420 - val_loss: 2.3806\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 2.29606\n",
      "Number of images: 45\n",
      "Presicion: 0.6015\n",
      "Recall: 0.3238\n",
      "F1 score: 0.421\n",
      "Epoch 79/1000\n",
      "420/420 [==============================] - 219s 521ms/step - loss: 2.0795 - val_loss: 2.3775\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 2.29606\n",
      "Number of images: 45\n",
      "Presicion: 0.6028\n",
      "Recall: 0.34\n",
      "F1 score: 0.4348\n",
      "Epoch 80/1000\n",
      "420/420 [==============================] - 217s 516ms/step - loss: 2.0756 - val_loss: 2.3234\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 2.29606\n",
      "Number of images: 45\n",
      "Presicion: 0.6099\n",
      "Recall: 0.2875\n",
      "F1 score: 0.3908\n",
      "Epoch 81/1000\n",
      "420/420 [==============================] - 218s 520ms/step - loss: 2.1095 - val_loss: 2.3633\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 2.29606\n",
      "Number of images: 45\n",
      "Presicion: 0.5865\n",
      "Recall: 0.338\n",
      "F1 score: 0.4288\n",
      "Epoch 82/1000\n",
      "420/420 [==============================] - 219s 521ms/step - loss: 2.1124 - val_loss: 2.3479\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 2.29606\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Number of images: 45\n",
      "Presicion: 0.5871\n",
      "Recall: 0.2986\n",
      "F1 score: 0.3958\n",
      "Epoch 83/1000\n",
      "420/420 [==============================] - 219s 521ms/step - loss: 2.0408 - val_loss: 2.2761\n",
      "\n",
      "Epoch 00083: val_loss improved from 2.29606 to 2.27609, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.6034\n",
      "Recall: 0.3072\n",
      "F1 score: 0.4071\n",
      "Epoch 84/1000\n",
      "420/420 [==============================] - 219s 521ms/step - loss: 2.0197 - val_loss: 2.3052\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 2.27609\n",
      "Number of images: 45\n",
      "Presicion: 0.595\n",
      "Recall: 0.339\n",
      "F1 score: 0.4319\n",
      "Epoch 85/1000\n",
      "420/420 [==============================] - 218s 518ms/step - loss: 2.0252 - val_loss: 2.2880\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 2.27609\n",
      "Number of images: 45\n",
      "Presicion: 0.5979\n",
      "Recall: 0.3588\n",
      "F1 score: 0.4484\n",
      "Epoch 86/1000\n",
      "420/420 [==============================] - 218s 520ms/step - loss: 2.0193 - val_loss: 2.4125\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 2.27609\n",
      "Number of images: 45\n",
      "Presicion: 0.6265\n",
      "Recall: 0.344\n",
      "F1 score: 0.4442\n",
      "Epoch 87/1000\n",
      "420/420 [==============================] - 220s 523ms/step - loss: 2.0256 - val_loss: 2.3264\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 2.27609\n",
      "Number of images: 45\n",
      "Presicion: 0.618\n",
      "Recall: 0.3241\n",
      "F1 score: 0.4252\n",
      "Epoch 88/1000\n",
      "420/420 [==============================] - 217s 516ms/step - loss: 2.0019 - val_loss: 2.3670\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 2.27609\n",
      "Number of images: 45\n",
      "Presicion: 0.6281\n",
      "Recall: 0.3669\n",
      "F1 score: 0.4632\n",
      "Epoch 89/1000\n",
      "420/420 [==============================] - 217s 517ms/step - loss: 1.9938 - val_loss: 2.2921\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 2.27609\n",
      "Number of images: 45\n",
      "Presicion: 0.6191\n",
      "Recall: 0.3323\n",
      "F1 score: 0.4325\n",
      "Epoch 90/1000\n",
      "420/420 [==============================] - 218s 519ms/step - loss: 1.9836 - val_loss: 2.3183\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 2.27609\n",
      "Number of images: 45\n",
      "Presicion: 0.6127\n",
      "Recall: 0.3286\n",
      "F1 score: 0.4278\n",
      "Epoch 91/1000\n",
      "420/420 [==============================] - 219s 520ms/step - loss: 1.9814 - val_loss: 2.2880\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 2.27609\n",
      "Number of images: 45\n",
      "Presicion: 0.6189\n",
      "Recall: 0.3045\n",
      "F1 score: 0.4082\n",
      "Epoch 92/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/420 [==============================] - 219s 522ms/step - loss: 1.9514 - val_loss: 2.3145\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 2.27609\n",
      "Number of images: 45\n",
      "Presicion: 0.5984\n",
      "Recall: 0.2989\n",
      "F1 score: 0.3986\n",
      "Epoch 93/1000\n",
      "420/420 [==============================] - 217s 518ms/step - loss: 1.9636 - val_loss: 2.2999\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 2.27609\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Number of images: 45\n",
      "Presicion: 0.6048\n",
      "Recall: 0.3264\n",
      "F1 score: 0.424\n",
      "Epoch 94/1000\n",
      "420/420 [==============================] - 219s 521ms/step - loss: 1.9611 - val_loss: 2.2882\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 2.27609\n",
      "Number of images: 45\n",
      "Presicion: 0.5983\n",
      "Recall: 0.327\n",
      "F1 score: 0.4229\n",
      "Epoch 95/1000\n",
      "420/420 [==============================] - 219s 521ms/step - loss: 1.9527 - val_loss: 2.2730\n",
      "\n",
      "Epoch 00095: val_loss improved from 2.27609 to 2.27299, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.6149\n",
      "Recall: 0.3387\n",
      "F1 score: 0.4368\n",
      "Epoch 96/1000\n",
      "420/420 [==============================] - 219s 521ms/step - loss: 1.9163 - val_loss: 2.2972\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 2.27299\n",
      "Number of images: 45\n",
      "Presicion: 0.6109\n",
      "Recall: 0.3638\n",
      "F1 score: 0.456\n",
      "Epoch 97/1000\n",
      "420/420 [==============================] - 218s 520ms/step - loss: 1.9297 - val_loss: 2.3210\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 2.27299\n",
      "Number of images: 45\n",
      "Presicion: 0.6276\n",
      "Recall: 0.3594\n",
      "F1 score: 0.4571\n",
      "Epoch 98/1000\n",
      "420/420 [==============================] - 219s 520ms/step - loss: 1.9204 - val_loss: 2.3058\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 2.27299\n",
      "Number of images: 45\n",
      "Presicion: 0.6394\n",
      "Recall: 0.3466\n",
      "F1 score: 0.4495\n",
      "Epoch 99/1000\n",
      "420/420 [==============================] - 219s 522ms/step - loss: 1.9151 - val_loss: 2.2912\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 2.27299\n",
      "Number of images: 45\n",
      "Presicion: 0.6241\n",
      "Recall: 0.3218\n",
      "F1 score: 0.4247\n",
      "Epoch 100/1000\n",
      "420/420 [==============================] - 219s 520ms/step - loss: 1.9360 - val_loss: 2.2917\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 2.27299\n",
      "Number of images: 45\n",
      "Presicion: 0.626\n",
      "Recall: 0.3356\n",
      "F1 score: 0.4369\n",
      "Epoch 101/1000\n",
      "420/420 [==============================] - 219s 521ms/step - loss: 1.9608 - val_loss: 2.2758\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 2.27299\n",
      "Number of images: 45\n",
      "Presicion: 0.6208\n",
      "Recall: 0.353\n",
      "F1 score: 0.45\n",
      "Epoch 102/1000\n",
      "420/420 [==============================] - 217s 518ms/step - loss: 1.9230 - val_loss: 2.2936\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 2.27299\n",
      "Number of images: 45\n",
      "Presicion: 0.6294\n",
      "Recall: 0.3309\n",
      "F1 score: 0.4337\n",
      "Epoch 103/1000\n",
      "420/420 [==============================] - 220s 523ms/step - loss: 1.9168 - val_loss: 2.2769\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 2.27299\n",
      "Number of images: 45\n",
      "Presicion: 0.614\n",
      "Recall: 0.3126\n",
      "F1 score: 0.4143\n",
      "Epoch 104/1000\n",
      "420/420 [==============================] - 219s 522ms/step - loss: 1.9116 - val_loss: 2.2914\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 2.27299\n",
      "Number of images: 45\n",
      "Presicion: 0.6173\n",
      "Recall: 0.3246\n",
      "F1 score: 0.4255\n",
      "Epoch 105/1000\n",
      "420/420 [==============================] - 221s 525ms/step - loss: 1.9175 - val_loss: 2.3007\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 2.27299\n",
      "\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Number of images: 45\n",
      "Presicion: 0.6202\n",
      "Recall: 0.343\n",
      "F1 score: 0.4417\n",
      "Epoch 106/1000\n",
      "420/420 [==============================] - 219s 521ms/step - loss: 1.9138 - val_loss: 2.2826\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 2.27299\n",
      "Number of images: 45\n",
      "Presicion: 0.62\n",
      "Recall: 0.3533\n",
      "F1 score: 0.4501\n",
      "Epoch 107/1000\n",
      "420/420 [==============================] - 220s 523ms/step - loss: 1.8983 - val_loss: 2.2731\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 2.27299\n",
      "Number of images: 45\n",
      "Presicion: 0.6324\n",
      "Recall: 0.3436\n",
      "F1 score: 0.4453\n",
      "Epoch 00107: early stopping\n"
     ]
    }
   ],
   "source": [
    "# If you're resuming a previous training, set `initial_epoch` and `final_epoch` accordingly.\n",
    "initial_epoch   = 0\n",
    "final_epoch     = 1000\n",
    "steps_per_epoch = 420\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset learning rate to 0.001\n",
    "adam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images into memory: 100%|██████████| 210/210 [00:07<00:00, 27.53it/s]\n",
      "Loading images into memory: 100%|██████████| 45/45 [00:01<00:00, 27.65it/s]\n",
      "Number of images in the training dataset:\t   210\n",
      "Number of images in the validation dataset:\t    45\n"
     ]
    }
   ],
   "source": [
    "# 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
    "\n",
    "# Optional: If you have enough memory, consider loading the images into memory for the reasons explained above.\n",
    "\n",
    "train_dataset = DataGenerator(load_images_into_memory=True, hdf5_dataset_path=None)\n",
    "val_dataset = DataGenerator(load_images_into_memory=True, hdf5_dataset_path=None)\n",
    "\n",
    "# 2: Parse the image and label lists for the training and validation datasets.\n",
    "\n",
    "# TODO: Set the paths to your dataset here.\n",
    "\n",
    "# Images\n",
    "# images_dir = data_path + 'udacity_driving_datasets'\n",
    "# images_dir = data_path + 'pascal_dataset'\n",
    "images_dir = data_path + 'images'\n",
    "\n",
    "# Ground truth\n",
    "# train_labels_filename = preprocess_path + '/udc.csv'\n",
    "# train_labels_filename = preprocess_path + '/cic_pascal_train.csv'\n",
    "# val_labels_filename   = preprocess_path + '/cic_pascal_val.csv'\n",
    "# train_labels_filename = preprocess_path + '/pascal_train.csv'\n",
    "# val_labels_filename   = preprocess_path + '/pascal_val.csv'\n",
    "train_labels_filename = preprocess_path + '/cic_train.csv'\n",
    "val_labels_filename   = preprocess_path + '/cic_val.csv'\n",
    "\n",
    "train_dataset.parse_csv(images_dir=images_dir,\n",
    "                        labels_filename=train_labels_filename,\n",
    "                        input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'], # This is the order of the first six columns in the CSV file that contains the labels for your dataset. If your labels are in XML format, maybe the XML parser will be helpful, check the documentation.\n",
    "                        include_classes='all')\n",
    "\n",
    "val_dataset.parse_csv(images_dir=images_dir,\n",
    "                      labels_filename=val_labels_filename,\n",
    "                      input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'],\n",
    "                      include_classes='all')\n",
    "\n",
    "# Optional: Convert the dataset into an HDF5 dataset. This will require more disk space, but will\n",
    "# speed up the training. Doing this is not relevant in case you activated the `load_images_into_memory`\n",
    "# option in the constructor, because in that cas the images are in memory already anyway. If you don't\n",
    "# want to create HDF5 datasets, comment out the subsequent two function calls.\n",
    "\n",
    "# train_dataset.create_hdf5_dataset(file_path='dataset_udacity_traffic_train.h5',\n",
    "#                                   resize=False,\n",
    "#                                   variable_image_size=True,\n",
    "#                                   verbose=True)\n",
    "\n",
    "# val_dataset.create_hdf5_dataset(file_path='dataset_udacity_traffic_val.h5',\n",
    "#                                 resize=False,\n",
    "#                                 variable_image_size=True,\n",
    "#                                 verbose=True)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training dataset:\t   210\n",
      "Number of images in the validation dataset:\t    45\n"
     ]
    }
   ],
   "source": [
    "# 3: Set the batch size.\n",
    "batch_size = 30 # Change the batch size if you like, or if you run into GPU memory issues.\n",
    "\n",
    "# 4: Set the image transformations for pre-processing and data augmentation options.\n",
    "# For the training generator:\n",
    "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
    "                                            img_width=img_width,\n",
    "                                            background=mean_color)\n",
    "\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [model.get_layer('conv13_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv17_mbox_conf').output_shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[ssd_data_augmentation],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model callbacks.\n",
    "\n",
    "# TODO: Set the filepath under which you want to save the model.\n",
    "model_checkpoint = ModelCheckpoint(filepath='/home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_cic_13_17.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "# model_checkpoint.best = 2.567152314715915\n",
    "\n",
    "csv_logger = CSVLogger(filename=data_path + 'history/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_cic_13_17.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0.0,\n",
    "                               patience=12,\n",
    "                               verbose=1)\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                         factor=0.5,\n",
    "                                         patience=10,\n",
    "                                         verbose=1,\n",
    "                                         min_delta=0.001,\n",
    "                                         cooldown=0,\n",
    "                                         min_lr=0.000001)\n",
    "\n",
    "f1_callback = f1_call(0.20, \n",
    "                      0.45, \n",
    "                      200, \n",
    "                      normalize_coords, \n",
    "                      img_height, \n",
    "                      img_width, \n",
    "                      val_images_320, \n",
    "                      label_val, (1, 2300, 14),\n",
    "                      data_path + 'history/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_cic_f1_13_17.csv')\n",
    "\n",
    "# learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule, verbose=1)\n",
    "\n",
    "\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             early_stopping,\n",
    "#              learning_rate_scheduler]\n",
    "             reduce_learning_rate,\n",
    "             f1_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "15/15 [==============================] - 66s 4s/step - loss: 3.1761 - val_loss: 3.0613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.06134, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_cic_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.6038\n",
      "Recall: 0.4329\n",
      "F1 score: 0.5042\n",
      "Improve F1 score from -inf to 0.5042286696256516\n",
      "Epoch 2/1000\n",
      "15/15 [==============================] - 62s 4s/step - loss: 2.8066 - val_loss: 2.9720\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.06134 to 2.97199, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_cic_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.6098\n",
      "Recall: 0.3887\n",
      "F1 score: 0.4748\n",
      "Epoch 3/1000\n",
      "15/15 [==============================] - 63s 4s/step - loss: 2.5938 - val_loss: 2.7973\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.97199 to 2.79731, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_cic_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.6319\n",
      "Recall: 0.3776\n",
      "F1 score: 0.4728\n",
      "Epoch 4/1000\n",
      "15/15 [==============================] - 62s 4s/step - loss: 2.7344 - val_loss: 2.9937\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.79731\n",
      "Number of images: 45\n",
      "Presicion: 0.6488\n",
      "Recall: 0.3836\n",
      "F1 score: 0.4822\n",
      "Epoch 5/1000\n",
      "15/15 [==============================] - 61s 4s/step - loss: 2.7685 - val_loss: 3.0620\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.79731\n",
      "Number of images: 45\n",
      "Presicion: 0.6471\n",
      "Recall: 0.3395\n",
      "F1 score: 0.4453\n",
      "Epoch 6/1000\n",
      "15/15 [==============================] - 64s 4s/step - loss: 2.6128 - val_loss: 3.1523\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.79731\n",
      "Number of images: 45\n",
      "Presicion: 0.6537\n",
      "Recall: 0.3441\n",
      "F1 score: 0.4509\n",
      "Epoch 7/1000\n",
      "15/15 [==============================] - 65s 4s/step - loss: 2.5410 - val_loss: 2.9773\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.79731\n",
      "Number of images: 45\n",
      "Presicion: 0.6449\n",
      "Recall: 0.3763\n",
      "F1 score: 0.4753\n",
      "Epoch 8/1000\n",
      "15/15 [==============================] - 65s 4s/step - loss: 2.5045 - val_loss: 2.7767\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.79731 to 2.77665, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_cic_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.6647\n",
      "Recall: 0.4152\n",
      "F1 score: 0.5111\n",
      "Improve F1 score from 0.5042286696256516 to 0.5110999507384426\n",
      "Epoch 9/1000\n",
      "15/15 [==============================] - 63s 4s/step - loss: 2.3930 - val_loss: 2.9179\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.77665\n",
      "Number of images: 45\n",
      "Presicion: 0.6592\n",
      "Recall: 0.3705\n",
      "F1 score: 0.4744\n",
      "Epoch 10/1000\n",
      "15/15 [==============================] - 60s 4s/step - loss: 2.2795 - val_loss: 2.8045\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.77665\n",
      "Number of images: 45\n",
      "Presicion: 0.6847\n",
      "Recall: 0.3691\n",
      "F1 score: 0.4796\n",
      "Epoch 11/1000\n",
      "15/15 [==============================] - 64s 4s/step - loss: 2.3683 - val_loss: 2.9068\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.77665\n",
      "Number of images: 45\n",
      "Presicion: 0.6403\n",
      "Recall: 0.367\n",
      "F1 score: 0.4666\n",
      "Epoch 12/1000\n",
      "15/15 [==============================] - 63s 4s/step - loss: 2.1885 - val_loss: 2.9598\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.77665\n",
      "Number of images: 45\n",
      "Presicion: 0.6482\n",
      "Recall: 0.3975\n",
      "F1 score: 0.4928\n",
      "Epoch 13/1000\n",
      "15/15 [==============================] - 65s 4s/step - loss: 2.3886 - val_loss: 2.8209\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.77665\n",
      "Number of images: 45\n",
      "Presicion: 0.6605\n",
      "Recall: 0.4493\n",
      "F1 score: 0.5348\n",
      "Improve F1 score from 0.5110999507384426 to 0.5347810919218112\n",
      "Epoch 14/1000\n",
      "15/15 [==============================] - 64s 4s/step - loss: 2.2080 - val_loss: 2.7872\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.77665\n",
      "Number of images: 45\n",
      "Presicion: 0.6763\n",
      "Recall: 0.4608\n",
      "F1 score: 0.5481\n",
      "Improve F1 score from 0.5347810919218112 to 0.5481188534437184\n",
      "Epoch 15/1000\n",
      "15/15 [==============================] - 63s 4s/step - loss: 2.3518 - val_loss: 2.6508\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.77665 to 2.65079, saving model to /home/aldo/Documents/weights/models/ssdlite320_mobilenetv2/ssdlite320_mobilenetv2_pascal_cic_13_17.h5\n",
      "Number of images: 45\n",
      "Presicion: 0.6774\n",
      "Recall: 0.4107\n",
      "F1 score: 0.5114\n",
      "Epoch 16/1000\n",
      "15/15 [==============================] - 63s 4s/step - loss: 2.1387 - val_loss: 2.6758\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.65079\n",
      "Number of images: 45\n",
      "Presicion: 0.6967\n",
      "Recall: 0.3779\n",
      "F1 score: 0.49\n",
      "Epoch 17/1000\n",
      "15/15 [==============================] - 65s 4s/step - loss: 2.1935 - val_loss: 2.8464\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.65079\n",
      "Number of images: 45\n",
      "Presicion: 0.669\n",
      "Recall: 0.4199\n",
      "F1 score: 0.516\n",
      "Epoch 18/1000\n",
      "15/15 [==============================] - 60s 4s/step - loss: 2.2764 - val_loss: 2.7444\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.65079\n",
      "Number of images: 45\n",
      "Presicion: 0.6597\n",
      "Recall: 0.3736\n",
      "F1 score: 0.477\n",
      "Epoch 19/1000\n",
      "15/15 [==============================] - 66s 4s/step - loss: 2.2627 - val_loss: 2.8832\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.65079\n",
      "Number of images: 45\n",
      "Presicion: 0.6831\n",
      "Recall: 0.412\n",
      "F1 score: 0.514\n",
      "Epoch 20/1000\n",
      "15/15 [==============================] - 60s 4s/step - loss: 2.2483 - val_loss: 2.7221\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.65079\n",
      "Number of images: 45\n",
      "Presicion: 0.6676\n",
      "Recall: 0.3662\n",
      "F1 score: 0.473\n",
      "Epoch 21/1000\n",
      "15/15 [==============================] - 63s 4s/step - loss: 2.0525 - val_loss: 2.8368\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.65079\n",
      "Number of images: 45\n",
      "Presicion: 0.6052\n",
      "Recall: 0.3478\n",
      "F1 score: 0.4417\n",
      "Epoch 22/1000\n",
      "15/15 [==============================] - 63s 4s/step - loss: 2.1447 - val_loss: 2.8342\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.65079\n",
      "Number of images: 45\n",
      "Presicion: 0.6124\n",
      "Recall: 0.376\n",
      "F1 score: 0.4659\n",
      "Epoch 23/1000\n",
      "15/15 [==============================] - 63s 4s/step - loss: 2.2530 - val_loss: 2.8383\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.65079\n",
      "Number of images: 45\n",
      "Presicion: 0.6827\n",
      "Recall: 0.3521\n",
      "F1 score: 0.4645\n",
      "Epoch 24/1000\n",
      "15/15 [==============================] - 62s 4s/step - loss: 1.9493 - val_loss: 2.8335\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.65079\n",
      "Number of images: 45\n",
      "Presicion: 0.6704\n",
      "Recall: 0.3785\n",
      "F1 score: 0.4839\n",
      "Epoch 25/1000\n",
      "15/15 [==============================] - 64s 4s/step - loss: 2.2391 - val_loss: 2.8286\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.65079\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Number of images: 45\n",
      "Presicion: 0.6845\n",
      "Recall: 0.3944\n",
      "F1 score: 0.5005\n",
      "Epoch 26/1000\n",
      "15/15 [==============================] - 60s 4s/step - loss: 2.0275 - val_loss: 2.8529\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.65079\n",
      "Number of images: 45\n",
      "Presicion: 0.6823\n",
      "Recall: 0.3793\n",
      "F1 score: 0.4876\n",
      "Epoch 27/1000\n",
      "15/15 [==============================] - 66s 4s/step - loss: 2.2119 - val_loss: 2.8133\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.65079\n",
      "Number of images: 45\n",
      "Presicion: 0.7008\n",
      "Recall: 0.37\n",
      "F1 score: 0.4843\n",
      "Epoch 00027: early stopping\n"
     ]
    }
   ],
   "source": [
    "# If you're resuming a previous training, set `initial_epoch` and `final_epoch` accordingly.\n",
    "initial_epoch   = 0\n",
    "final_epoch     = 1000\n",
    "steps_per_epoch = 15\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test2]",
   "language": "python",
   "name": "conda-env-test2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
